#!/usr/bin/env python3
"""
RNSR Comparison Benchmarks

Compares RNSR against baseline approaches:
1. Naive RAG (chunk + embed + retrieve)
2. Long-context LLM (no RAG)
3. RNSR (hierarchical + ToT)

Metrics:
- Retrieval Precision/Recall
- Answer Quality (LLM-as-judge)
- Hallucination Detection
- Citation Accuracy
- Response Time
- Cost per Query

Usage:
    python scripts/compare_benchmarks.py --pdf document.pdf --queries queries.json
    python scripts/compare_benchmarks.py --quick  # Use sample data
"""

import argparse
import json
import os
import re
import sys
import time
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any

sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress
except ImportError:
    import subprocess
    subprocess.run([sys.executable, "-m", "pip", "install", "rich"])
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress

console = Console()

# Cached LLM instance to avoid re-initialization on every call
_cached_llm = None

def get_cached_llm():
    """Get a cached LLM instance to avoid re-initialization overhead."""
    global _cached_llm
    if _cached_llm is None:
        from rnsr.llm import get_llm
        _cached_llm = get_llm()
    return _cached_llm


# =============================================================================
# Benchmark Data Structures
# =============================================================================

@dataclass
class BenchmarkQuery:
    """A query with ground truth for evaluation."""
    question: str
    ground_truth_answer: str = ""
    relevant_sections: list[str] = field(default_factory=list)
    expected_entities: list[str] = field(default_factory=list)
    difficulty: str = "medium"  # easy, medium, hard


@dataclass 
class BenchmarkResult:
    """Result from a single benchmark run."""
    method: str
    query: str
    answer: str
    time_sec: float
    sources_used: int = 0
    
    # Quality metrics (0-1)
    answer_relevance: float = 0.0
    answer_correctness: float = 0.0
    hallucination_score: float = 0.0  # 0 = no hallucination, 1 = all hallucinated
    citation_accuracy: float = 0.0
    
    # Cost
    tokens_used: int = 0
    estimated_cost: float = 0.0


@dataclass
class ComparisonReport:
    """Full comparison report."""
    document: str
    num_queries: int
    results: list[BenchmarkResult] = field(default_factory=list)
    summary: dict = field(default_factory=dict)


# =============================================================================
# Baseline: Naive RAG
# =============================================================================

class NaiveRAGBaseline:
    """
    Simple chunk-based RAG for comparison.
    
    This represents the typical approach:
    1. Split document into fixed-size chunks
    2. Embed chunks
    3. Retrieve top-k by similarity
    4. Generate answer from chunks
    """
    
    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50, top_k: int = 5):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.top_k = top_k
        self.chunks = []
        self.embeddings = []
        
    def ingest(self, text: str):
        """Chunk the document."""
        self.chunks = self._chunk_text(text)
        console.print(f"[dim]NaiveRAG: Created {len(self.chunks)} chunks[/dim]")
        
    def _chunk_text(self, text: str) -> list[str]:
        """Split text into overlapping chunks."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]
            if chunk.strip():
                chunks.append(chunk)
            start = end - self.chunk_overlap
        return chunks
    
    def query(self, question: str) -> dict:
        """Answer using naive retrieval."""
        start = time.time()
        
        # Simple keyword matching for retrieval (in real impl, use embeddings)
        question_words = set(question.lower().split())
        scored_chunks = []
        
        for i, chunk in enumerate(self.chunks):
            chunk_words = set(chunk.lower().split())
            overlap = len(question_words & chunk_words)
            scored_chunks.append((overlap, i, chunk))
        
        # Get top-k chunks
        scored_chunks.sort(reverse=True)
        top_chunks = [c[2] for c in scored_chunks[:self.top_k]]
        
        # Generate answer
        context = "\n\n---\n\n".join(top_chunks)
        
        llm = get_cached_llm()
        prompt = f"""Based on the following context, answer the question.

Context:
{context}

Question: {question}

Answer:"""
        
        response = llm.complete(prompt)
        answer = str(response)
        
        elapsed = time.time() - start
        
        return {
            "answer": answer,
            "time_sec": elapsed,
            "sources": len(top_chunks),
            "method": "naive_rag",
        }


# =============================================================================
# Baseline: Long-Context LLM
# =============================================================================

class LongContextBaseline:
    """
    Just stuff the whole document into the LLM context.
    
    This tests whether structure matters - if the doc fits in context,
    does hierarchical understanding still help?
    """
    
    def __init__(self, max_chars: int = 50000):  # Reduced to show truncation
        self.max_chars = max_chars
        self.document = ""
        self.truncated = False
        self.original_length = 0
        
    def ingest(self, text: str):
        """Store document (truncated if needed)."""
        self.original_length = len(text)
        self.document = text[:self.max_chars]
        if len(text) > self.max_chars:
            self.truncated = True
            pct_lost = ((len(text) - self.max_chars) / len(text)) * 100
            console.print(f"[red]LongContext: TRUNCATED! Lost {pct_lost:.0f}% of document ({len(text):,} -> {self.max_chars:,} chars)[/red]")
        else:
            console.print(f"[dim]LongContext: Full document ({len(text):,} chars)[/dim]")
    
    def query(self, question: str) -> dict:
        """Answer by stuffing whole doc in context."""
        start = time.time()
        
        llm = get_cached_llm()
        prompt = f"""Read the following document and answer the question.

Document:
{self.document}

Question: {question}

Answer:"""
        
        response = llm.complete(prompt)
        answer = str(response)
        
        elapsed = time.time() - start
        
        return {
            "answer": answer,
            "time_sec": elapsed,
            "sources": 1,
            "method": "long_context",
        }


# =============================================================================
# RNSR System
# =============================================================================

class RNSRSystem:
    """RNSR with full hierarchical processing and knowledge graph."""
    
    def __init__(self):
        self.tree = None
        self.skeleton = None
        self.kv_store = None
        self.navigator = None
        self.knowledge_graph = None
        
    def ingest(self, pdf_path: Path):
        """Ingest document with hierarchical processing."""
        from rnsr.ingestion.pipeline import ingest_document
        from rnsr.indexing import build_skeleton_index
        
        result = ingest_document(pdf_path)
        self.tree = result.tree
        self.skeleton, self.kv_store = build_skeleton_index(self.tree)
        
        console.print(f"[dim]RNSR: {self.tree.total_nodes} nodes[/dim]")
    
    def ingest_text(self, text: str):
        """Ingest from text (for comparison with baselines)."""
        from rnsr.models import DocumentNode, DocumentTree
        
        # Parse markdown headers properly
        lines = text.split('\n')
        root = DocumentNode(id="root", header="Document", content="", level=0)
        
        # Track the node stack for building hierarchy
        node_stack = [root]  # Stack of (node, level)
        current_content = []
        node_counter = 0
        
        for line in lines:
            # Check for markdown headers: #, ##, ###, etc.
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
            
            if header_match:
                # Save accumulated content to current node
                if current_content and len(node_stack) > 1:
                    node_stack[-1].content = '\n'.join(current_content).strip()
                current_content = []
                
                level = len(header_match.group(1))
                header_text = header_match.group(2).strip()
                
                # Create new node
                new_node = DocumentNode(
                    id=f"section_{node_counter}",
                    header=header_text[:100],
                    content="",
                    level=level,
                )
                node_counter += 1
                
                # Find correct parent - pop until we find a node with lower level
                while len(node_stack) > 1 and node_stack[-1].level >= level:
                    node_stack.pop()
                
                # Add as child of current top of stack
                node_stack[-1].children.append(new_node)
                node_stack.append(new_node)
            else:
                # Accumulate content
                current_content.append(line)
        
        # Save final content
        if current_content and len(node_stack) > 1:
            node_stack[-1].content = '\n'.join(current_content).strip()
        elif current_content:
            # No headers found - create a single section
            node = DocumentNode(
                id="section_0",
                header="Content",
                content='\n'.join(current_content).strip(),
                level=1,
            )
            root.children.append(node)
        
        # Count total nodes
        def count_nodes(node):
            return 1 + sum(count_nodes(child) for child in node.children)
        
        total = count_nodes(root)
        self.tree = DocumentTree(root=root, id="doc", total_nodes=total)
        
        from rnsr.indexing import build_skeleton_index
        self.skeleton, self.kv_store = build_skeleton_index(self.tree)
        
        # Build knowledge graph for entity extraction
        self._build_knowledge_graph()
        
        console.print(f"[dim]RNSR: {total} nodes[/dim]")
    
    def _build_knowledge_graph(self):
        """Build knowledge graph with extracted entities."""
        try:
            from rnsr.indexing.knowledge_graph import KnowledgeGraph
            
            # Create in-memory knowledge graph
            self.knowledge_graph = KnowledgeGraph(":memory:")
            
            # Extract and add entities from each node
            for node_id, node in self.skeleton.items():
                content = self.kv_store.get(node_id) or ""
                full_text = f"{node.header}\n{content}"
                
                # Extract entities (company names, people, etc.)
                entities = self._extract_entities(full_text, node.header)
                
                for entity in entities:
                    self.knowledge_graph.add_entity(
                        name=entity["name"],
                        entity_type=entity["type"],
                        doc_id="benchmark_doc",
                        node_id=node_id,
                        properties={"header": node.header},
                    )
            
            entity_count = len(self.knowledge_graph.get_all_entities())
            console.print(f"[dim]RNSR: Extracted {entity_count} entities[/dim]")
            
        except Exception as e:
            console.print(f"[yellow]Knowledge graph build failed: {e}[/yellow]")
            self.knowledge_graph = None
    
    def _extract_entities(self, text: str, header: str) -> list[dict]:
        """Extract named entities from text."""
        entities = []
        
        # Pattern for company names (Inc., LLC, Corp., Ltd., etc.)
        company_pattern = r'([A-Z][A-Za-z\s]+(?:Inc\.|LLC|Corp\.|Ltd\.|Corporation|Company))'
        for match in re.finditer(company_pattern, text):
            entities.append({
                "name": match.group(1).strip(),
                "type": "ORGANIZATION",
            })
        
        # Pattern for roles in quotes (e.g., "Client", "Provider")
        role_pattern = r'"([A-Z][A-Za-z]+)"'
        for match in re.finditer(role_pattern, text):
            role = match.group(1)
            if role in ["Client", "Provider", "Contractor", "Vendor", "Customer"]:
                entities.append({
                    "name": role,
                    "type": "ROLE",
                })
        
        # Pattern for monetary values
        money_pattern = r'\$[\d,]+(?:\.\d{2})?(?:\s*(?:USD|dollars?))?'
        for match in re.finditer(money_pattern, text, re.IGNORECASE):
            entities.append({
                "name": match.group(0),
                "type": "MONEY",
            })
        
        # Pattern for dates
        date_pattern = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4}'
        for match in re.finditer(date_pattern, text):
            entities.append({
                "name": match.group(0),
                "type": "DATE",
            })
        
        return entities
    
    def query(self, question: str) -> dict:
        """Answer using RNSR navigation."""
        from rnsr.agent.rlm_navigator import RLMNavigator
        
        start = time.time()
        
        if self.navigator is None:
            from rnsr.agent.rlm_navigator import RLMConfig
            
            # Disable overly strict verification that rejects valid answers
            config = RLMConfig(
                enable_verification=False,  # The critic is too aggressive
            )
            
            self.navigator = RLMNavigator(
                skeleton=self.skeleton,
                kv_store=self.kv_store,
                knowledge_graph=self.knowledge_graph,
                config=config,
            )
            # Use cached LLM to avoid re-initialization
            llm = get_cached_llm()
            self.navigator.set_llm_function(lambda p: str(llm.complete(p)))
        
        result = self.navigator.navigate(question)
        
        elapsed = time.time() - start
        
        # Include confidence in result
        confidence = result.get("confidence", 0.5)
        answer = result.get("answer", "")
        
        return {
            "answer": answer,
            "time_sec": elapsed,
            "sources": len(result.get("sources", [])),
            "method": "rnsr",
            "confidence": confidence,
        }


# =============================================================================
# Evaluation Metrics
# =============================================================================

def is_not_found_response(answer: str) -> bool:
    """Check if the answer indicates information was not found."""
    not_found_phrases = [
        "unable to find",
        "not found in the document",
        "information is not found",
        "could not find",
        "no information",
        "not available",
        "cannot determine",
        "not mentioned",
        "does not contain",
    ]
    answer_lower = answer.lower()
    return any(phrase in answer_lower for phrase in not_found_phrases)


def evaluate_answer_quality(
    question: str,
    answer: str,
    ground_truth: str,
    confidence: float | None = None,
) -> dict:
    """
    Evaluate answer quality using LLM-as-judge with BINARY correctness.
    
    Uses a simple yes/no check: Does the answer contain the expected information?
    This handles long-form answers that may include additional context.
    
    Returns:
    - relevance: 1.0 if answer addresses the question, 0.0 otherwise
    - correctness: 1.0 if answer contains expected info, 0.0 otherwise
    - completeness: Same as correctness for simplicity
    - honesty: 0.5 default
    """
    if not ground_truth:
        return {"relevance": 0.5, "correctness": 0.5, "completeness": 0.5, "honesty": 0.5}
    
    # Handle "not found" responses specially
    if is_not_found_response(answer):
        return {
            "relevance": 0.3,
            "correctness": 0.0,
            "completeness": 0.0,
            "honesty": 1.0,
        }
    
    prompt = f"""You are evaluating if an answer correctly addresses a question.

QUESTION: {question}

EXPECTED ANSWER (key information that should be present):
{ground_truth}

GENERATED ANSWER:
{answer}

Does the generated answer contain the key information from the expected answer?
- The answer may include ADDITIONAL correct information (that's fine)
- The answer may use different wording (that's fine)  
- The key facts/values from the expected answer should be present

Think step by step:
1. What are the key facts in the expected answer?
2. Are those facts present in the generated answer?

Respond with ONLY a JSON object:
{{"is_correct": 1, "reasoning": "brief explanation"}}

Use is_correct=1 if the answer contains the expected information, is_correct=0 if it's missing or wrong."""

    try:
        llm = get_cached_llm()
        response = llm.complete(prompt)
        response_text = str(response)
        
        # Parse JSON
        json_match = re.search(r'\{[^}]+\}', response_text, re.DOTALL)
        if json_match:
            result_json = json.loads(json_match.group())
            is_correct = int(result_json.get("is_correct", 0))
            
            return {
                "relevance": 1.0 if is_correct else 0.5,
                "correctness": float(is_correct),
                "completeness": float(is_correct),
                "honesty": 0.5,
            }
    except Exception as e:
        console.print(f"[yellow]Evaluation error: {e}[/yellow]")
    
    return {"relevance": 0.5, "correctness": 0.5, "completeness": 0.5, "honesty": 0.5}


def detect_hallucination(answer: str, source_text: str) -> float:
    """
    Detect hallucination using BINARY check.
    
    Returns: 0.0 if answer is grounded in source, 1.0 if it contains fabricated info
    """
    prompt = f"""You are checking if an answer contains FABRICATED information.

SOURCE DOCUMENT (the only valid source of truth):
{source_text[:4000]}

GENERATED ANSWER:
{answer}

Check if the answer makes any FACTUAL CLAIMS that are NOT supported by the source document.

IMPORTANT:
- Quoting directly from the source = NOT hallucination
- Paraphrasing information from the source = NOT hallucination  
- Providing additional context that IS in the source = NOT hallucination
- Making up facts/numbers NOT in the source = HALLUCINATION
- Claiming things that contradict the source = HALLUCINATION

Does this answer contain fabricated/unsupported factual claims?

Respond with ONLY a JSON object:
{{"has_hallucination": 0, "reasoning": "brief explanation"}}

Use has_hallucination=0 if all claims are grounded in the source.
Use has_hallucination=1 if the answer contains fabricated information."""

    try:
        llm = get_cached_llm()
        response = llm.complete(prompt)
        response_text = str(response)
        
        json_match = re.search(r'\{[^}]+\}', response_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            has_hallucination = int(result.get("has_hallucination", 0))
            return float(has_hallucination)
    except Exception:
        pass
    
    return 0.0  # Default to no hallucination if evaluation fails


# =============================================================================
# Benchmark Runner
# =============================================================================

def run_comparison(
    document_text: str,
    queries: list[BenchmarkQuery],
    pdf_path: Path = None,
) -> ComparisonReport:
    """Run full comparison benchmark."""
    
    console.print("\n[bold blue]ðŸ“Š Running Comparison Benchmark[/bold blue]\n")
    
    # Initialize systems
    naive_rag = NaiveRAGBaseline()
    long_context = LongContextBaseline()
    rnsr = RNSRSystem()
    
    # Ingest document
    console.print("[bold]Ingesting document...[/bold]")
    
    naive_rag.ingest(document_text)
    long_context.ingest(document_text)
    
    if pdf_path:
        rnsr.ingest(pdf_path)
    else:
        rnsr.ingest_text(document_text)
    
    console.print()
    
    # Run queries
    results = []
    
    with Progress() as progress:
        task = progress.add_task("Running queries...", total=len(queries) * 3)
        
        for query in queries:
            for system, name in [
                (naive_rag, "Naive RAG"),
                (long_context, "Long Context"),
                (rnsr, "RNSR"),
            ]:
                try:
                    result = system.query(query.question)
                    
                    # Evaluate quality (pass confidence for RNSR)
                    confidence = result.get("confidence") if name == "RNSR" else None
                    quality = evaluate_answer_quality(
                        query.question,
                        result["answer"],
                        query.ground_truth_answer,
                        confidence=confidence,
                    )
                    
                    # Detect hallucination
                    hallucination = detect_hallucination(
                        result["answer"],
                        document_text[:5000],
                    )
                    
                    # For "not found" responses, hallucination should be 0
                    if is_not_found_response(result["answer"]):
                        hallucination = 0.0
                    
                    benchmark_result = BenchmarkResult(
                        method=result["method"],
                        query=query.question,
                        answer=result["answer"],
                        time_sec=result["time_sec"],
                        sources_used=result.get("sources", 0),
                        answer_relevance=quality["relevance"],
                        answer_correctness=quality["correctness"],
                        hallucination_score=hallucination,
                    )
                    results.append(benchmark_result)
                    
                except Exception as e:
                    console.print(f"[red]Error with {name}: {e}[/red]")
                
                progress.advance(task)
    
    # Generate report
    report = ComparisonReport(
        document=pdf_path.name if pdf_path else "text_input",
        num_queries=len(queries),
        results=results,
    )
    
    # Calculate summary statistics
    report.summary = calculate_summary(results)
    
    return report


def calculate_summary(results: list[BenchmarkResult]) -> dict:
    """Calculate summary statistics by method."""
    summary = {}
    
    methods = set(r.method for r in results)
    
    for method in methods:
        method_results = [r for r in results if r.method == method]
        
        if not method_results:
            continue
            
        summary[method] = {
            "avg_time_sec": sum(r.time_sec for r in method_results) / len(method_results),
            "avg_relevance": sum(r.answer_relevance for r in method_results) / len(method_results),
            "avg_correctness": sum(r.answer_correctness for r in method_results) / len(method_results),
            "avg_hallucination": sum(r.hallucination_score for r in method_results) / len(method_results),
            "total_sources": sum(r.sources_used for r in method_results),
            "num_queries": len(method_results),
        }
    
    return summary


def display_report(report: ComparisonReport):
    """Display benchmark report."""
    
    console.print("\n[bold blue]ðŸ“ˆ Benchmark Results[/bold blue]\n")
    
    # Summary table
    table = Table(title="Performance Comparison")
    table.add_column("Method", style="cyan")
    table.add_column("Avg Time", style="yellow")
    table.add_column("Relevance", style="green")
    table.add_column("Correctness", style="green")
    table.add_column("Hallucination", style="red")
    
    for method, stats in report.summary.items():
        # Format method name nicely
        method_name = {
            "naive_rag": "Naive RAG",
            "long_context": "Long Context",
            "rnsr": "RNSR",
        }.get(method, method)
        
        # Color code hallucination (lower is better)
        hall_color = "green" if stats["avg_hallucination"] < 0.3 else "yellow" if stats["avg_hallucination"] < 0.5 else "red"
        
        table.add_row(
            method_name,
            f"{stats['avg_time_sec']:.2f}s",
            f"{stats['avg_relevance']:.0%}",
            f"{stats['avg_correctness']:.0%}",
            f"[{hall_color}]{stats['avg_hallucination']:.0%}[/{hall_color}]",
        )
    
    console.print(table)
    
    # Winner analysis
    console.print("\n[bold]Analysis:[/bold]\n")
    
    if "rnsr" in report.summary and "naive_rag" in report.summary:
        rnsr = report.summary["rnsr"]
        naive = report.summary["naive_rag"]
        
        if rnsr["avg_correctness"] > naive["avg_correctness"]:
            improvement = (rnsr["avg_correctness"] - naive["avg_correctness"]) / naive["avg_correctness"] * 100
            console.print(f"  âœ… RNSR correctness is [green]{improvement:.0f}% better[/green] than Naive RAG")
        
        if rnsr["avg_hallucination"] < naive["avg_hallucination"]:
            reduction = (naive["avg_hallucination"] - rnsr["avg_hallucination"]) / naive["avg_hallucination"] * 100
            console.print(f"  âœ… RNSR reduces hallucination by [green]{reduction:.0f}%[/green]")


def create_sample_queries() -> list[BenchmarkQuery]:
    """Create sample queries for benchmarking."""
    return [
        BenchmarkQuery(
            question="What is the main topic of this document?",
            ground_truth_answer="",  # Will be evaluated without ground truth
            difficulty="easy",
        ),
        BenchmarkQuery(
            question="What are the key findings or conclusions?",
            ground_truth_answer="",
            difficulty="medium",
        ),
        BenchmarkQuery(
            question="Summarize the methodology or approach described.",
            ground_truth_answer="",
            difficulty="medium",
        ),
        BenchmarkQuery(
            question="What specific numbers or statistics are mentioned?",
            ground_truth_answer="",
            difficulty="hard",
        ),
        BenchmarkQuery(
            question="What are the main sections of this document?",
            ground_truth_answer="",
            difficulty="easy",
        ),
    ]


def run_quick_benchmark():
    """Run benchmark on sample contract."""
    
    samples_dir = Path(__file__).parent.parent / "samples"
    sample_contract = samples_dir / "sample_contract.md"
    
    if not sample_contract.exists():
        console.print("[red]Sample documents not found.[/red]")
        return
    
    document_text = sample_contract.read_text()
    
    # Create queries specific to the contract with accurate ground truths
    queries = [
        BenchmarkQuery(
            question="Who are the parties to this contract?",
            ground_truth_answer="Acme Technologies Inc. (Client) and DataSoft Solutions LLC (Provider)",
            difficulty="easy",
        ),
        BenchmarkQuery(
            question="What is the total contract value?",
            ground_truth_answer="$750,000 USD (Seven Hundred Fifty Thousand Dollars)",
            difficulty="easy",
        ),
        BenchmarkQuery(
            question="What is the deliverable for Phase 1 and when is it due?",
            ground_truth_answer="Phase 1: Discovery - Requirements documentation, due February 28, 2024",
            difficulty="medium",
        ),
        BenchmarkQuery(
            question="What are the termination conditions?",
            ground_truth_answer="Either party may terminate with 60 days written notice. Termination for cause requires 30 days notice with opportunity to cure.",
            difficulty="medium",
        ),
    ]
    
    report = run_comparison(document_text, queries)
    display_report(report)
    
    # Save report
    report_path = Path("benchmark_comparison_report.json")
    with open(report_path, "w") as f:
        json.dump(asdict(report), f, indent=2, default=str)
    
    console.print(f"\n[green]ðŸ“„ Report saved to: {report_path}[/green]")


def generate_large_document(num_sections: int = 50) -> tuple[str, list[BenchmarkQuery]]:
    """
    Generate a large synthetic document that will challenge Long Context.
    
    The key answer is buried in section 42 - Long Context will truncate before reaching it.
    """
    sections = []
    
    # Header
    sections.append("# COMPREHENSIVE CORPORATE POLICY MANUAL\n\n")
    sections.append("**Document ID:** CPM-2024-MASTER\n")
    sections.append("**Effective Date:** January 1, 2024\n")
    sections.append("**Total Sections:** " + str(num_sections) + "\n\n---\n\n")
    
    # Generate many filler sections
    topics = [
        ("Workplace Safety", "safety protocols, emergency procedures, hazard reporting"),
        ("Information Security", "data protection, password policies, access control"),
        ("Human Resources", "hiring procedures, performance reviews, benefits"),
        ("Financial Controls", "expense reporting, budget approval, audit procedures"),
        ("Quality Assurance", "testing protocols, defect tracking, compliance"),
        ("Customer Service", "support procedures, escalation paths, SLA requirements"),
        ("Project Management", "planning phases, resource allocation, risk management"),
        ("Legal Compliance", "regulatory requirements, contract review, liability"),
        ("Environmental Policy", "sustainability goals, waste management, carbon footprint"),
        ("Vendor Management", "procurement, supplier evaluation, contract negotiation"),
    ]
    
    for i in range(num_sections):
        topic_name, topic_desc = topics[i % len(topics)]
        section_num = i + 1
        
        # THE KEY SECTION - buried deep in the document
        if section_num == 42:
            sections.append(f"## Section {section_num}: Executive Compensation\n\n")
            sections.append("### 42.1 CEO Compensation Package\n\n")
            sections.append("The Chief Executive Officer shall receive:\n")
            sections.append("- **Base Salary:** $2,750,000 annually\n")
            sections.append("- **Performance Bonus:** Up to 150% of base salary\n")
            sections.append("- **Stock Options:** 500,000 shares vesting over 4 years\n")
            sections.append("- **Signing Bonus:** $1,500,000 (one-time)\n\n")
            sections.append("### 42.2 Clawback Provisions\n\n")
            sections.append("All incentive compensation is subject to clawback in cases of:\n")
            sections.append("- Financial restatement due to misconduct\n")
            sections.append("- Violation of company policies\n")
            sections.append("- Termination for cause\n\n")
            sections.append("---\n\n")
        else:
            # Filler content
            sections.append(f"## Section {section_num}: {topic_name} - Part {(i // len(topics)) + 1}\n\n")
            sections.append(f"### {section_num}.1 Overview\n\n")
            sections.append(f"This section covers {topic_desc}. ")
            sections.append("All employees must familiarize themselves with these policies. ")
            sections.append("Compliance is mandatory and subject to regular audits.\n\n")
            
            sections.append(f"### {section_num}.2 Procedures\n\n")
            sections.append(f"1. Review the {topic_name.lower()} guidelines quarterly\n")
            sections.append(f"2. Report any concerns to the {topic_name} committee\n")
            sections.append(f"3. Complete mandatory training within 30 days\n")
            sections.append(f"4. Document all incidents per standard protocol\n\n")
            
            sections.append(f"### {section_num}.3 Responsibilities\n\n")
            sections.append(f"| Role | Responsibility |\n")
            sections.append(f"|------|----------------|\n")
            sections.append(f"| Manager | Ensure team compliance with all {topic_name.lower()} requirements |\n")
            sections.append(f"| Employee | Follow all documented procedures and report violations |\n")
            sections.append(f"| Auditor | Verify compliance through regular assessments |\n")
            sections.append(f"| Director | Provide oversight and approve policy changes |\n\n")
            
            # Add more filler to make document larger
            sections.append(f"### {section_num}.4 Detailed Guidelines\n\n")
            sections.append(f"The following detailed guidelines apply to all {topic_name.lower()} activities:\n\n")
            for j in range(5):
                sections.append(f"**Guideline {section_num}.4.{j+1}:** All personnel must adhere to the established ")
                sections.append(f"protocols for {topic_desc}. Failure to comply may result in disciplinary action ")
                sections.append(f"as outlined in the employee handbook. Regular training sessions are conducted ")
                sections.append(f"quarterly to ensure all staff remain current on best practices and regulatory ")
                sections.append(f"requirements. Documentation of compliance must be maintained for audit purposes.\n\n")
            
            sections.append("---\n\n")
    
    document = "".join(sections)
    
    # Queries - the key one requires finding Section 42
    queries = [
        BenchmarkQuery(
            question="What is the CEO's base salary according to this policy manual?",
            ground_truth_answer="$2,750,000 annually",
            difficulty="hard",  # Hard because it's buried deep
        ),
        BenchmarkQuery(
            question="What is the CEO's signing bonus?",
            ground_truth_answer="$1,500,000 (one-time)",
            difficulty="hard",
        ),
        BenchmarkQuery(
            question="What are the clawback provisions for executive compensation?",
            ground_truth_answer="Financial restatement due to misconduct, violation of company policies, termination for cause",
            difficulty="hard",
        ),
        BenchmarkQuery(
            question="How many stock options does the CEO receive?",
            ground_truth_answer="500,000 shares vesting over 4 years",
            difficulty="hard",
        ),
    ]
    
    console.print(f"[dim]Generated document: {len(document):,} chars, {num_sections} sections[/dim]")
    console.print(f"[yellow]Key info is in Section 42 - Long Context will truncate before reaching it![/yellow]")
    
    return document, queries


def run_large_benchmark():
    """Run benchmark on large synthetic document to show RNSR's advantage."""
    console.print("\n[bold]Generating large document to challenge Long Context...[/bold]\n")
    
    document_text, queries = generate_large_document(num_sections=50)
    
    report = run_comparison(document_text, queries)
    display_report(report)
    
    # Save report
    report_path = Path("benchmark_large_report.json")
    with open(report_path, "w") as f:
        json.dump(asdict(report), f, indent=2, default=str)
    
    console.print(f"\n[green]ðŸ“„ Report saved to: {report_path}[/green]")


def main():
    parser = argparse.ArgumentParser(description="RNSR Comparison Benchmarks")
    parser.add_argument("--pdf", type=Path, help="PDF to benchmark")
    parser.add_argument("--queries", type=Path, help="JSON file with queries")
    parser.add_argument("--quick", action="store_true", help="Quick benchmark with samples")
    parser.add_argument("--large", action="store_true", help="Large document benchmark (shows RNSR advantage)")
    
    args = parser.parse_args()
    
    console.print(Panel.fit(
        "[bold cyan]RNSR Comparison Benchmark[/bold cyan]\n"
        "Comparing RNSR vs Naive RAG vs Long Context",
        border_style="cyan",
    ))
    
    if args.large:
        run_large_benchmark()
    elif args.quick or (not args.pdf and not args.queries):
        run_quick_benchmark()
    elif args.pdf:
        # Load PDF and run benchmark
        if not args.pdf.exists():
            console.print(f"[red]File not found: {args.pdf}[/red]")
            return 1
        
        # Read PDF text
        import fitz
        doc = fitz.open(args.pdf)
        text = "\n\n".join(page.get_text() for page in doc)
        
        queries = create_sample_queries()
        if args.queries and args.queries.exists():
            with open(args.queries) as f:
                query_data = json.load(f)
                queries = [BenchmarkQuery(**q) for q in query_data]
        
        report = run_comparison(text, queries, args.pdf)
        display_report(report)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
